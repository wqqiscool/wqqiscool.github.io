---
layout:     post
title:     Binder通信驱动
subtitle:    底层探究
date:       2020-04-08
author:     wqq
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - android
    - 驱动
    - Linux
    - Binder
    - ipc通信
---
##### binder底层源码来源
  当然是github官网linux源码目录：[linux内核binder驱动目录](https://github.com/torvalds/linux/tree/master/drivers/android)
+ biner.c
+ binder_alloc.c
+ binder_alloc.h
+ ...
##### 何为“驱动”
  （英语：device driver），简称驱动程序（driver），是一个允许高端（High level）电脑软件（computer software）与硬件（hardware）交互的程序，这种程序创建了一个硬件与硬件，或硬件与软件沟通的接口，经由主板上的总线（bus）或其它沟通子系统（subsystem）与硬件形成连接的机制，这样的机制使得硬件设备（device）上的数据交换成为可能。

   依据不同的计算机体系结构与操作系统差异平台，驱动程序经历了8位（8-bit）、16位（16-bit）、32位（32-bit）、64位（64-bit）变迁，这是为了调和操作系统与驱动程序之间的依存关系，例如在Windows 3.11的16位操作系统时代，大部分的驱动程序都是16位，到了32位的Windows XP则使用32位驱动程序（微软提供了Windows Driver Model可实现driver），至于64位的Linux或是Windows平台上，就必须使用64位的驱动程序（WDM与WDF皆可实现64位驱动程序）。来源：[维基百科](https://zh.wikipedia.org/wiki/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F)
###### 基于linux驱动基本类型
+ 字符设备（binder驱动就是此类型号，处理内存操作)
+ 块设备 
+ 网络接口
###### binder驱动概述
binder驱动是基于openbinder，专为android而用，与其他linux驱动架构一样，是以[misc]()设备进行注册，是*虚拟的字符设备*,不直接操作硬件，只是对设备内存处理。主要有以下功能：
+ 驱动设备的初始化(binder_init)
+ 打开驱动(binder_open)：每个应用只打开一次binder驱动，依稀记得是在[Zygote启动]()这个老祖宗里面有个打开binder线程池
+ 映射(binder_mmap)：这是binder-ipc通信只复制一次的关键
+ 内存读写操作(binder_ioctl)：前边铺垫就是为了这一步，这也是ipc的核心操作
###### binder_init
代码如下：

        printf();//test
  	static int __init binder_init(void)
	 {
	int ret;
	char *device_name, *device_tmp;
	struct binder_device *device;
	struct hlist_node *tmp;
	char *device_names = NULL;
	ret = binder_alloc_shrinker_init();
	if (ret)
	return ret;
	atomic_set(&binder_transaction_log.cur, ~0U);
	atomic_set(&binder_transaction_log_failed.cur, ~0U);
	binder_debugfs_dir_entry_root = debugfs_create_dir("binder", NULL);
	if (binder_debugfs_dir_entry_root)
		binder_debugfs_dir_entry_proc = debugfs_create_dir("proc",
						 binder_debugfs_dir_entry_root);
	if (binder_debugfs_dir_entry_root) {
		debugfs_create_file("state,0444,binder_debugfs_dir_entry_,NULL,&binder_state_fops);
		debugfs_create_file("stats",
				    0444,
				    binder_debugfs_dir_entry_root,
				    NULL,
				    &binder_stats_fops);
		debugfs_create_file("transactions",
				    0444,
				    binder_debugfs_dir_entry_root,
				    NULL,
				    &binder_transactions_fops);
		debugfs_create_file("transaction_log",
				    0444,
				    binder_debugfs_dir_entry_root,
				    &binder_transaction_log,
				    &binder_transaction_log_fops);
		debugfs_create_file("failed_transaction_log",
				    0444,
				    binder_debugfs_dir_entry_root,
				    &binder_transaction_log_failed,
				    &binder_transaction_log_fops);
	}
	if (!IS_ENABLED(CONFIG_ANDROID_BINDERFS) &&
	    strcmp(binder_devices_param, "") != 0) {
		/*
		* Copy the module_parameter string, because we don't want to
		* tokenize it in-place.
		 */
		device_names = kstrdup(binder_devices_param, GFP_KERNEL);
		if (!device_names) {
			ret = -ENOMEM;
			goto err_alloc_device_names_failed;
		}
		device_tmp = device_names;
		while ((device_name = strsep(&device_tmp, ","))) {
			ret = init_binder_device(device_name);
			if (ret)
				goto err_init_binder_device_failed;
		}
	}
	ret = init_binderfs();
	if (ret)
		goto err_init_binder_device_failed;
	return ret;
	err_init_binder_device_failed:
	hlist_for_each_entry_safe(device, tmp, &binder_devices, hlist) {
		misc_deregister(&device->miscdev);
		hlist_del(&device->hlist);
		kfree(device);
	}
	kfree(device_names);
	err_alloc_device_names_failed:
	debugfs_remove_recursive(binder_debugfs_dir_entry_root);
	return ret;
	}


解析：注意不同版本的内核代码有所不同，此处注意init_binder_device(bindername)方法
如下：详见--->[binder.c](https://github.com/torvalds/linux/blob/master/drivers/android/binder.c)



	static int __init init_binder_device(const char *name)
	{
	int ret;
	struct binder_device *binder_device;

	binder_device = kzalloc(sizeof(*binder_device), GFP_KERNEL);
	if (!binder_device)
		return -ENOMEM;

	binder_device->miscdev.fops = &binder_fops;
	binder_device->miscdev.minor = MISC_DYNAMIC_MINOR;
	binder_device->miscdev.name = name;

	refcount_set(&binder_device->ref, 1);
	binder_device->context.binder_context_mgr_uid = INVALID_UID;
	binder_device->context.name = name;
	mutex_init(&binder_device->context.context_mgr_node_lock);

	ret = misc_register(&binder_device->miscdev);
	if (ret < 0) {
		kfree(binder_device);
		return ret;
	}

	hlist_add_head(&binder_device->hlist, &binder_devices);

	return ret;
	}
 
        const struct file_operations binder_fops = {
	.owner = THIS_MODULE,
	.poll = binder_poll,
	.unlocked_ioctl = binder_ioctl,
	.compat_ioctl = compat_ptr_ioctl,
	.mmap = binder_mmap,
	.open = binder_open,
	.flush = binder_flush,
	.release = binder_release,
	};


注意“file_operations”结构体：
    Linux使用file_operations结构访问驱动程序的函数，这个结构的每一个成员的名字都对应着一个调用。
   用户进程利用在对设备文件进行诸如read/write操作的时候，系统调用通过设备文件的主设备号找到相应的设备驱动程序，然后读取这个数据结构相应的函数指针，接着把控制权交给该函数，这是Linux的设备驱动程序工作的基本原理,代码结构体如下：参见[file_operation结构体](https://github.com/torvalds/linux/blob/master/include/linux/fs.h)

	struct file_operations { 
　　	struct module *owner;//拥有该结构的模块的指针，一般为THIS_MODULES  
   	loff_t (*llseek) (struct file *, loff_t, int);//用来修改文件当前的读写位置  
   	ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);//从设备中同步读取数据   
   	ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);//向设备发送数据  
   	ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t);//初始化一个异步的读取操作   
   	ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t);//初始化一个异步的写入操作   
	int (*readdir) (struct file *, void *, filldir_t);//仅用于读取目录，对于设备文件，该字段为NULL   
   	unsigned int (*poll) (struct file *, struct poll_table_struct *); //轮询函数，判断目前是否可以进行非阻塞的读写或写入   
　	int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long); //执行设备I/O控制命令   
　　	long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); //不使用BLK文件系统，将使用此种函数指针代替ioctl  
　　	long (*compat_ioctl) (struct file *, unsigned int, unsigned long); //在64位系统上，32位的ioctl调用将使用此函数指针代替   
　　	int (*mmap) (struct file *, struct vm_area_struct *); //用于请求将设备内存映射到进程地址空间  
　　	int (*open) (struct inode *, struct file *); //打开   
　　	int (*flush) (struct file *, fl_owner_t id);   
　　	int (*release) (struct inode *, struct file *); //关闭   
　　	int (*fsync) (struct file *, struct dentry *, int datasync); //刷新待处理的数据   
　　	int (*aio_fsync) (struct kiocb *, int datasync); //异步刷新待处理的数据   
　　	int (*fasync) (int, struct file *, int); //通知设备FASYNC标志发生变化   
　　	int (*lock) (struct file *, int, struct file_lock *);   
　　	ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int);   
　　	unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);   
　　	int (*check_flags)(int);   
　　	int (*flock) (struct file *, int, struct file_lock *);  
　　	ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);  
　　	ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);   
　　	int (*setlease)(struct file *, long, struct file_lock **);   
	};

此时注意下misc_register(....)方法，注册misc设备，何为[misc].()?
源码参见：[misc源码](https://github.com/torvalds/linux/blob/master/include/linux/miscdevice.h)
 
	....
	struct miscdevice  {
	int minor;
	const char *name;
	const struct file_operations *fops;
	struct list_head list;
	struct device *parent;
	struct device *this_device;
	const struct attribute_group **groups;
	const char *nodename;
	umode_t mode;
	};

	extern int misc_register(struct miscdevice *misc);
	extern void misc_deregister(struct miscdevice *misc);

        ....

[misc_register方法](https://github.com/torvalds/linux/blob/master/drivers/char/misc.c)
###### binder_open
打开设备驱动，驱动为此生成binder_proc结构体，并将当前进程相关信息赋值给此结构体，并加入到全局链表中`binder_procs`中去。

	static int binder_open(struct inode *nodp, struct file *filp)
{
	struct binder_proc *proc, *itr;
	struct binder_device *binder_dev;
	struct binderfs_info *info;
	struct dentry *binder_binderfs_dir_entry_proc = NULL;
	bool existing_pid = false;

	binder_debug(BINDER_DEBUG_OPEN_CLOSE, "%s: %d:%d\n", __func__,
		     current->group_leader->pid, current->pid);

	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
	if (proc == NULL)
		return -ENOMEM;
	spin_lock_init(&proc->inner_lock);
	spin_lock_init(&proc->outer_lock);
	get_task_struct(current->group_leader);
	proc->tsk = current->group_leader;
	INIT_LIST_HEAD(&proc->todo);
	proc->default_priority = task_nice(current);
	/* binderfs stashes devices in i_private */
	if (is_binderfs_device(nodp)) {
		binder_dev = nodp->i_private;
		info = nodp->i_sb->s_fs_info;
		binder_binderfs_dir_entry_proc = info->proc_log_dir;
	} else {
		binder_dev = container_of(filp->private_data,
					  struct binder_device, miscdev);
	}
	refcount_inc(&binder_dev->ref);
	proc->context = &binder_dev->context;
	binder_alloc_init(&proc->alloc);

	binder_stats_created(BINDER_STAT_PROC);
	proc->pid = current->group_leader->pid;
	INIT_LIST_HEAD(&proc->delivered_death);
	INIT_LIST_HEAD(&proc->waiting_threads);
	filp->private_data = proc;

	mutex_lock(&binder_procs_lock);
	hlist_for_each_entry(itr, &binder_procs, proc_node) {
		if (itr->pid == proc->pid) {
			existing_pid = true;
			break;
		}
	}
	hlist_add_head(&proc->proc_node, &binder_procs);
	mutex_unlock(&binder_procs_lock);

	if (binder_debugfs_dir_entry_proc && !existing_pid) {
		char strbuf[11];

		snprintf(strbuf, sizeof(strbuf), "%u", proc->pid);
		/*
		 * proc debug entries are shared between contexts.
		 * Only create for the first PID to avoid debugfs log spamming
		 * The printing code will anyway print all contexts for a given
		 * PID so this is not a problem.
		 */
		proc->debugfs_entry = debugfs_create_file(strbuf, 0444,
			binder_debugfs_dir_entry_proc,
			(void *)(unsigned long)proc->pid,
			&proc_fops);
	}

	if (binder_binderfs_dir_entry_proc && !existing_pid) {
		char strbuf[11];
		struct dentry *binderfs_entry;

		snprintf(strbuf, sizeof(strbuf), "%u", proc->pid);
		/*
		 * Similar to debugfs, the process specific log file is shared
		 * between contexts. Only create for the first PID.
		 * This is ok since same as debugfs, the log file will contain
		 * information on all contexts of a given PID.
		 */
		binderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,
			strbuf, &proc_fops, (void *)(unsigned long)proc->pid);
		if (!IS_ERR(binderfs_entry)) {
			proc->binderfs_entry = binderfs_entry;
		} else {
			int error;

			error = PTR_ERR(binderfs_entry);
			pr_warn("Unable to create file %s in binderfs (error %d)\n",
				strbuf, error);
		}
	}

	return 0;
	}

###### binder_mmap
内核虚拟空间申请与用户内存相同大小的内存，再申请1page大小的物理内存，然后将此真实的物理内存同时映射到内核虚拟地址空间和用户虚拟用户空间，从而实现了用户空间的buffer和内核的buffer同步，说白了就是两个指针同时指向了同一块物理内存。

	static int binder_mmap(struct file *filp, struct vm_area_struct *vma)
{
	int ret;
	struct binder_proc *proc = filp->private_data;
	const char *failure_string;

	if (proc->tsk != current->group_leader)
		return -EINVAL;

	binder_debug(BINDER_DEBUG_OPEN_CLOSE,
		     "%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\n",
		     __func__, proc->pid, vma->vm_start, vma->vm_end,
		     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,
		     (unsigned long)pgprot_val(vma->vm_page_prot));

	if (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {
		ret = -EPERM;
		failure_string = "bad vm_flags";
		goto err_bad_arg;
	}
	vma->vm_flags |= VM_DONTCOPY | VM_MIXEDMAP;
	vma->vm_flags &= ~VM_MAYWRITE;

	vma->vm_ops = &binder_vm_ops;
	vma->vm_private_data = proc;

	ret = binder_alloc_mmap_handler(&proc->alloc, vma);
	if (ret)
		return ret;
	return 0;
	err_bad_arg:
	pr_err("%s: %d %lx-%lx %s failed %d\n", __func__,
	       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);
	return ret;
	}

注意某一行` ret = binder_alloc_mmap_handler(&proc->alloc, vma);  `
跟踪到`binder_alloc.c`文件中去：[linux /drivers/android/binder_alloc.c](https://github.com/torvalds/linux/blob/master/drivers/android/binder_alloc.c)

	/**
 	* binder_alloc_mmap_handler() - map virtual address space for proc
	 * @alloc:	alloc structure for this proc
 	* @vma:	vma passed to mmap()
 	*
 	* Called by binder_mmap() to initialize the space specified in
 	* vma for allocating binder buffers
 	*
 	* Return:
 	*      0 = success
 	*      -EBUSY = address space already mapped
 	*      -ENOMEM = failed to map memory to given address space
 	*/
	int binder_alloc_mmap_handler(struct binder_alloc *alloc,
			      struct vm_area_struct *vma)
	{
	int ret;
	const char *failure_string;
	struct binder_buffer *buffer;

	mutex_lock(&binder_alloc_mmap_lock);
	if (alloc->buffer_size) {
		ret = -EBUSY;
		failure_string = "already mapped";
		goto err_already_mapped;
	}
	alloc->buffer_size = min_t(unsigned long, vma->vm_end - vma->vm_start,
				   SZ_4M);
	mutex_unlock(&binder_alloc_mmap_lock);

	alloc->buffer = (void __user *)vma->vm_start;

	alloc->pages = kcalloc(alloc->buffer_size / PAGE_SIZE,
			       sizeof(alloc->pages[0]),
			       GFP_KERNEL);
	if (alloc->pages == NULL) {
		ret = -ENOMEM;
		failure_string = "alloc page array";
		goto err_alloc_pages_failed;
	}

	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
	if (!buffer) {
		ret = -ENOMEM;
		failure_string = "alloc buffer struct";
		goto err_alloc_buf_struct_failed;
	}

	buffer->user_data = alloc->buffer;
	list_add(&buffer->entry, &alloc->buffers);
	buffer->free = 1;
	binder_insert_free_buffer(alloc, buffer);
	alloc->free_async_space = alloc->buffer_size / 2;
	binder_alloc_set_vma(alloc, vma);
	mmgrab(alloc->vma_vm_mm);
	return 0;
	err_alloc_buf_struct_failed:
	kfree(alloc->pages);
	alloc->pages = NULL;
err_alloc_pages_failed:
	alloc->buffer = NULL;
	mutex_lock(&binder_alloc_mmap_lock);
	alloc->buffer_size = 0;
err_already_mapped:
	mutex_unlock(&binder_alloc_mmap_lock);
	binder_alloc_debug(BINDER_DEBUG_USER_ERROR,
			   "%s: %d %lx-%lx %s failed %d\n", __func__,
			   alloc->pid, vma->vm_start, vma->vm_end,
			   failure_string, ret);
	return ret;
	}

`binder_alloc`结构体

	/**
\ * struct binder_alloc - per-binder proc state for binder allocator
 \* @vma:                vm_area_struct passed to mmap_handler
 \*                      (invarient after mmap)
 \* @tsk:                tid for task that called init for this proc
 \*                      (invariant after init)
 \* @vma_vm_mm:          copy of vma->vm_mm (invarient after mmap)
 \* @buffer:             base of per-proc address space mapped via mmap
 \* @buffers:            list of all buffers for this proc
 \* @free_buffers:       rb tree of buffers available for allocation
 \*                      sorted by size
 \* @allocated_buffers:  rb tree of allocated buffers sorted by address
 \* @free_async_space:   VA space available for async buffers. This is
 \*                      initialized at mmap time to 1/2 the full VA space
 \* @pages:              array of binder_lru_page
 \* @buffer_size:        size of address space specified via mmap
 \* @pid:                pid for associated binder_proc (invariant after init)
 \* @pages_high:         high watermark of offset in @pages
 \*
 \* Bookkeeping structure for per-proc address space management for binder
 \* buffers. It is normally initialized during binder_init() and binder_mmap()
 \* calls. The address space is used for both user-visible buffers and for
 \* struct binder_buffer objects used to track the user buffers
 \*/
	struct binder_alloc {
	struct mutex mutex;
	struct vm_area_struct *vma;
	struct mm_struct *vma_vm_mm;
	void __user *buffer;
	struct list_head buffers;
	struct rb_root free_buffers;
	struct rb_root allocated_buffers;
	size_t free_async_space;
	struct binder_lru_page *pages;
	size_t buffer_size;
	uint32_t buffer_free;
	int pid;
	size_t pages_high;
	};

位于[linux/drivers/android/binder_alloc.h](https://github.com/torvalds/linux/blob/master/drivers/android/binder_alloc.h)
