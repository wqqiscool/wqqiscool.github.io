---
layout:     post
title:     Binder通信驱动
subtitle:    底层探究
date:       2020-04-08
author:     wqq
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - android
    - 驱动
    - Linux
    - Binder
    - ipc通信
---
###### binder底层源码来源
  当然是github官网linux源码目录：[linux内核binder驱动目录](https://github.com/torvalds/linux/tree/master/drivers/android),**这里勘正(打脸)一下，最好的地方肯定是google的android官方网站：[https://android.googlesource.com/kernel/](https://android.googlesource.com/kernel/) 针对不同的[cpu架构]()有不同的分支，我这边看了下[arm64](),[x86](),[x86_64]()，[msm](),不同分支下的driver目录，其中msm目录和前面github官网linux下的binder.c一致，其余的则不同，因此这个地发我们先存疑问，后边还是以arm64分析为主**
+ biner.c
+ ...
##### 何为“驱动”
  （英语：device driver），简称驱动程序（driver），是一个允许高端（High level）电脑软件（computer software）与硬件（hardware）交互的程序，这种程序创建了一个硬件与硬件，或硬件与软件沟通的接口，经由主板上的总线（bus）或其它沟通子系统（subsystem）与硬件形成连接的机制，这样的机制使得硬件设备（device）上的数据交换成为可能。

   依据不同的计算机体系结构与操作系统差异平台，驱动程序经历了8位（8-bit）、16位（16-bit）、32位（32-bit）、64位（64-bit）变迁，这是为了调和操作系统与驱动程序之间的依存关系，例如在Windows 3.11的16位操作系统时代，大部分的驱动程序都是16位，到了32位的Windows XP则使用32位驱动程序（微软提供了Windows Driver Model可实现driver），至于64位的Linux或是Windows平台上，就必须使用64位的驱动程序（WDM与WDF皆可实现64位驱动程序）。来源：[维基百科](https://zh.wikipedia.org/wiki/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F)
###### 基于linux驱动基本类型
+ 字符设备（binder驱动就是此类型号，处理内存操作)
+ 块设备 
+ 网络接口
###### binder驱动概述
binder驱动是基于openbinder，专为android而用，与其他linux驱动架构一样，是以[misc]()设备进行注册，是*虚拟的字符设备*,不直接操作硬件，只是对设备内存处理。主要有以下功能：
+ 驱动设备的初始化(binder_init)
+ 打开驱动(binder_open)：每个应用只打开一次binder驱动，依稀记得是在[Zygote启动]()这个老祖宗里面有个打开binder线程池
+ 映射(binder_mmap)：这是binder-ipc通信只复制一次的关键
+ 内存读写操作(binder_ioctl)：前边铺垫就是为了这一步，这也是ipc的核心操作

###### binder_init
进入到代码如下：这里存在疑问，binder_init 何时被调用？？？？？？


	static int __init binder_init(void)
	{
	int ret;
	binder_deferred_workqueue = create_singlethread_workqueue("binder");
	if (!binder_deferred_workqueue)
		return -ENOMEM;
	binder_debugfs_dir_entry_root = debugfs_create_dir("binder", NULL);
	if (binder_debugfs_dir_entry_root)
		binder_debugfs_dir_entry_proc = debugfs_create_dir("proc",
						 binder_debugfs_dir_entry_root);
	ret = misc_register(&binder_miscdev);
	if (binder_debugfs_dir_entry_root) {
		debugfs_create_file("state",
				    S_IRUGO,
				    binder_debugfs_dir_entry_root,
				    NULL,
				    &binder_state_fops);
		debugfs_create_file("stats",
				    S_IRUGO,
				    binder_debugfs_dir_entry_root,
				    NULL,
				    &binder_stats_fops);
		debugfs_create_file("transactions",
				    S_IRUGO,
				    binder_debugfs_dir_entry_root,
				    NULL,
				    &binder_transactions_fops);
		debugfs_create_file("transaction_log",
				    S_IRUGO,
				    binder_debugfs_dir_entry_root,
				    &binder_transaction_log,
				    &binder_transaction_log_fops);
		debugfs_create_file("failed_transaction_log",
				    S_IRUGO,
				    binder_debugfs_dir_entry_root,
				    &binder_transaction_log_failed,
				    &binder_transaction_log_fops);
	}
	return ret;
	}



解析：注意不同版本的内核代码有所不同，此处注意init_binder_device(bindername)方法
如下：详见--->[binder.c](https://android.googlesource.com/kernel/arm64/+/refs/tags/android-8.1.0_r0.6/drivers/android/binder.c)



	static int __init init_binder_device(const char *name)
	{
	int ret;
	struct binder_device *binder_device;

	binder_device = kzalloc(sizeof(*binder_device), GFP_KERNEL);
	if (!binder_device)
		return -ENOMEM;

	binder_device->miscdev.fops = &binder_fops;
	binder_device->miscdev.minor = MISC_DYNAMIC_MINOR;
	binder_device->miscdev.name = name;

	refcount_set(&binder_device->ref, 1);
	binder_device->context.binder_context_mgr_uid = INVALID_UID;
	binder_device->context.name = name;
	mutex_init(&binder_device->context.context_mgr_node_lock);

	ret = misc_register(&binder_device->miscdev);
	if (ret < 0) {
		kfree(binder_device);
		return ret;
	}

	hlist_add_head(&binder_device->hlist, &binder_devices);

	return ret;
	}
 
        const struct file_operations binder_fops = {
	.owner = THIS_MODULE,
	.poll = binder_poll,
	.unlocked_ioctl = binder_ioctl,
	.compat_ioctl = compat_ptr_ioctl,
	.mmap = binder_mmap,
	.open = binder_open,
	.flush = binder_flush,
	.release = binder_release,
	};


注意“file_operations”结构体：
    Linux使用file_operations结构访问驱动程序的函数，这个结构的每一个成员的名字都对应着一个调用。
   用户进程利用在对设备文件进行诸如read/write操作的时候，系统调用通过设备文件的主设备号找到相应的设备驱动程序，然后读取这个数据结构相应的函数指针，接着把控制权交给该函数，这是Linux的设备驱动程序工作的基本原理,代码结构体如下：参见[file_operation结构体](https://github.com/torvalds/linux/blob/master/include/linux/fs.h)

	struct file_operations { 
　　	struct module *owner;//拥有该结构的模块的指针，一般为THIS_MODULES  
   	loff_t (*llseek) (struct file *, loff_t, int);//用来修改文件当前的读写位置  
   	ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);//从设备中同步读取数据   
   	ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);//向设备发送数据  
   	ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t);//初始化一个异步的读取操作   
   	ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t);//初始化一个异步的写入操作   
	int (*readdir) (struct file *, void *, filldir_t);//仅用于读取目录，对于设备文件，该字段为NULL   
   	unsigned int (*poll) (struct file *, struct poll_table_struct *); //轮询函数，判断目前是否可以进行非阻塞的读写或写入   
　	int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long); //执行设备I/O控制命令   
　　	long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); //不使用BLK文件系统，将使用此种函数指针代替ioctl  
　　	long (*compat_ioctl) (struct file *, unsigned int, unsigned long); //在64位系统上，32位的ioctl调用将使用此函数指针代替   
　　	int (*mmap) (struct file *, struct vm_area_struct *); //用于请求将设备内存映射到进程地址空间  
　　	int (*open) (struct inode *, struct file *); //打开   
　　	int (*flush) (struct file *, fl_owner_t id);   
　　	int (*release) (struct inode *, struct file *); //关闭   
　　	int (*fsync) (struct file *, struct dentry *, int datasync); //刷新待处理的数据   
　　	int (*aio_fsync) (struct kiocb *, int datasync); //异步刷新待处理的数据   
　　	int (*fasync) (int, struct file *, int); //通知设备FASYNC标志发生变化   
　　	int (*lock) (struct file *, int, struct file_lock *);   
　　	ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int);   
　　	unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);   
　　	int (*check_flags)(int);   
　　	int (*flock) (struct file *, int, struct file_lock *);  
　　	ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);  
　　	ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);   
　　	int (*setlease)(struct file *, long, struct file_lock **);   
	};

此时注意下misc_register(....)方法，注册misc设备，何为[misc]()
源码参见：[misc源码](https://github.com/torvalds/linux/blob/master/include/linux/miscdevice.h)
 
	....
	struct miscdevice  {
	int minor;
	const char *name;
	const struct file_operations *fops;
	struct list_head list;
	struct device *parent;
	struct device *this_device;
	const struct attribute_group **groups;
	const char *nodename;
	umode_t mode;
	};

	extern int misc_register(struct miscdevice *misc);
	extern void misc_deregister(struct miscdevice *misc);

        ....

[misc_register方法](https://github.com/torvalds/linux/blob/master/drivers/char/misc.c)
###### binder_open
打开设备驱动，驱动为此生成binder_proc结构体，并将当前进程相关信息赋值给此结构体，并加入到全局链表中`binder_procs`中去。


	static int binder_open(struct inode *nodp, struct file *filp)
{
	struct binder_proc *proc;
	struct binder_device *binder_dev;
	binder_debug(BINDER_DEBUG_OPEN_CLOSE, "binder_open: %d:%d\n",
		     current->group_leader->pid, current->pid);
	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
	if (proc == NULL)
		return -ENOMEM;
	get_task_struct(current->group_leader);
	proc->tsk = current->group_leader;
	INIT_LIST_HEAD(&proc->todo);
	init_waitqueue_head(&proc->wait);
	proc->default_priority = task_nice(current);
	binder_dev = container_of(filp->private_data, struct binder_device,
				  miscdev);
	proc->context = &binder_dev->context;
	binder_lock(__func__);
	binder_stats_created(BINDER_STAT_PROC);
	hlist_add_head(&proc->proc_node, &binder_procs);
	proc->pid = current->group_leader->pid;
	INIT_LIST_HEAD(&proc->delivered_death);
	filp->private_data = proc;
	binder_unlock(__func__);
	if (binder_debugfs_dir_entry_proc) {
		char strbuf[11];
		snprintf(strbuf, sizeof(strbuf), "%u", proc->pid);
		/*
		 * proc debug entries are shared between contexts, so
		 * this will fail if the process tries to open the driver
		 * again with a different context. The priting code will
		 * anyway print all contexts that a given PID has, so this
		 * is not a problem.
		 */
		proc->debugfs_entry = debugfs_create_file(strbuf, S_IRUGO,
			binder_debugfs_dir_entry_proc,
			(void *)(unsigned long)proc->pid,
			&binder_proc_fops);
	}
	return 0;
	}


	
`binder_proc`这个玩意儿在*安卓6.0*之前是在driver/staging/android下面的binder.c里面，而后呢竟然去掉了，直接在driver/android/binder.c里面，真是说变就变。。。。。,且看


	struct binder_proc {
	struct hlist_node proc_node;
	struct rb_root threads;
	struct rb_root nodes;
	struct rb_root refs_by_desc;
	struct rb_root refs_by_node;
	int pid;
	struct vm_area_struct *vma;
	struct mm_struct *vma_vm_mm;
	struct task_struct *tsk;
	struct files_struct *files;
	struct hlist_node deferred_work_node;
	int deferred_work;
	void *buffer;
	ptrdiff_t user_buffer_offset;
	struct list_head buffers;
	struct rb_root free_buffers;
	struct rb_root allocated_buffers;
	size_t free_async_space;
	struct page **pages;
	size_t buffer_size;
	uint32_t buffer_free;
	struct list_head todo;
	wait_queue_head_t wait;
	struct binder_stats stats;
	struct list_head delivered_death;
	int max_threads;
	int requested_threads;
	int requested_threads_started;
	int ready_threads;
	long default_priority;
	struct dentry *debugfs_entry;
	struct binder_context *context;
};


注意成员`binder_context`

	struct binder_context {
	struct binder_node *binder_context_mgr_node;
	kuid_t binder_context_mgr_uid;
	const char *name;
};
struct binder_device {
	struct hlist_node hlist;
	struct miscdevice miscdev;
	struct binder_context context;
	};


注意几棵红黑树-`rb_root`，[还是专门学习一下把]()



###### binder_mmap
内核虚拟空间申请与用户内存相同大小的内存，再申请实际物理内存how big it is，然后将此真实的物理内存同时映射到内核虚拟地址空间和用户虚拟用户空间，从而实现了用户空间的buffer和内核的buffer同步，说白了就是两个指针同时指向了同一块物理内存。

	static int binder_mmap(struct file *filp, struct vm_area_struct *vma)
{
	int ret;
	struct vm_struct *area;
	struct binder_proc *proc = filp->private_data;
	const char *failure_string;
	struct binder_buffer *buffer;
	if (proc->tsk != current)
		return -EINVAL;
	if ((vma->vm_end - vma->vm_start) > SZ_4M)
		vma->vm_end = vma->vm_start + SZ_4M;
	binder_debug(BINDER_DEBUG_OPEN_CLOSE,
		     "binder_mmap: %d %lx-%lx (%ld K) vma %lx pagep %lx\n",
		     proc->pid, vma->vm_start, vma->vm_end,
		     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,
		     (unsigned long)pgprot_val(vma->vm_page_prot));
	if (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {
		ret = -EPERM;
		failure_string = "bad vm_flags";
		goto err_bad_arg;
	}
	vma->vm_flags = (vma->vm_flags | VM_DONTCOPY) & ~VM_MAYWRITE;
	mutex_lock(&binder_mmap_lock);
	if (proc->buffer) {
		ret = -EBUSY;
		failure_string = "already mapped";
		goto err_already_mapped;
	}
	area = get_vm_area(vma->vm_end - vma->vm_start, VM_IOREMAP);
	if (area == NULL) {
		ret = -ENOMEM;
		failure_string = "get_vm_area";
		goto err_get_vm_area_failed;
	}
	proc->buffer = area->addr;
	proc->user_buffer_offset = vma->vm_start - (uintptr_t)proc->buffer;
	mutex_unlock(&binder_mmap_lock);
#ifdef CONFIG_CPU_CACHE_VIPT
	if (cache_is_vipt_aliasing()) {
		while (CACHE_COLOUR((vma->vm_start ^ (uint32_t)proc->buffer))) {
			pr_info("binder_mmap: %d %lx-%lx maps %p bad alignment\n", proc->pid, vma->vm_start, vma->vm_end, proc->buffer);
			vma->vm_start += PAGE_SIZE;
		}
	}
#endif
	proc->pages = kzalloc(sizeof(proc->pages[0]) * ((vma->vm_end - vma->vm_start) / PAGE_SIZE), GFP_KERNEL);
	if (proc->pages == NULL) {
		ret = -ENOMEM;
		failure_string = "alloc page array";
		goto err_alloc_pages_failed;
	}
	proc->buffer_size = vma->vm_end - vma->vm_start;
	vma->vm_ops = &binder_vm_ops;
	vma->vm_private_data = proc;
	if (binder_update_page_range(proc, 1, proc->buffer, proc->buffer + PAGE_SIZE, vma)) {
		ret = -ENOMEM;
		failure_string = "alloc small buf";
		goto err_alloc_small_buf_failed;
	}
	buffer = proc->buffer;
	INIT_LIST_HEAD(&proc->buffers);
	list_add(&buffer->entry, &proc->buffers);
	buffer->free = 1;
	binder_insert_free_buffer(proc, buffer);
	proc->free_async_space = proc->buffer_size / 2;
	barrier();
	proc->files = get_files_struct(current);
	proc->vma = vma;
	proc->vma_vm_mm = vma->vm_mm;
	/*pr_info("binder_mmap: %d %lx-%lx maps %p\n",
		 proc->pid, vma->vm_start, vma->vm_end, proc->buffer);*/
	return 0;
err_alloc_small_buf_failed:
	kfree(proc->pages);
	proc->pages = NULL;
err_alloc_pages_failed:
	mutex_lock(&binder_mmap_lock);
	vfree(proc->buffer);
	proc->buffer = NULL;
err_get_vm_area_failed:
err_already_mapped:
	mutex_unlock(&binder_mmap_lock);
err_bad_arg:
	pr_err("binder_mmap: %d %lx-%lx %s failed %d\n",
	       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);
	return ret;
	}

**首先做的是在内核区申请一个与用户虚拟内存相同大小的` vm_struc area `通过` area = get_vm_area(vma->vm_end - vma->vm_start, VM_IOREMAP)`,然后将此块申请的内核虚拟内存地址赋值给`proc->buffer`--proc->buffer = area->addr;注意一个重要的方法`binder_update_page`**，我们进入到此方法去：也属于binder.c文件


	static int binder_update_page_range(struct binder_proc *proc, int allocate,
				    void *start, void *end,
				    struct vm_area_struct *vma)
{
	void *page_addr;
	unsigned long user_page_addr;
	struct page **page;
	struct mm_struct *mm;
	binder_debug(BINDER_DEBUG_BUFFER_ALLOC,
		     "%d: %s pages %p-%p\n", proc->pid,
		     allocate ? "allocate" : "free", start, end);
	if (end <= start)
		return 0;
	trace_binder_update_page_range(proc, allocate, start, end);
	if (vma)
		mm = NULL;
	else
		mm = get_task_mm(proc->tsk);
	if (mm) {
		down_write(&mm->mmap_sem);
		vma = proc->vma;
		if (vma && mm != proc->vma_vm_mm) {
			pr_err("%d: vma mm and task mm mismatch\n",
				proc->pid);
			vma = NULL;
		}
	}
	if (allocate == 0)
		goto free_range;
	if (vma == NULL) {
		pr_err("%d: binder_alloc_buf failed to map pages in userspace, no vma\n",
			proc->pid);
		goto err_no_vma;
	}
	for (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {
		int ret;
		page = &proc->pages[(page_addr - proc->buffer) / PAGE_SIZE];
		BUG_ON(*page);
		*page = alloc_page(GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO);
		if (*page == NULL) {
			pr_err("%d: binder_alloc_buf failed for page at %p\n",
				proc->pid, page_addr);
			goto err_alloc_page_failed;
		}
		ret = map_kernel_range_noflush((unsigned long)page_addr,
					PAGE_SIZE, PAGE_KERNEL, page);
		flush_cache_vmap((unsigned long)page_addr,
				(unsigned long)page_addr + PAGE_SIZE);
		if (ret != 1) {
			pr_err("%d: binder_alloc_buf failed to map page at %p in kernel\n",
			       proc->pid, page_addr);
			goto err_map_kernel_failed;
		}
		user_page_addr =
			(uintptr_t)page_addr + proc->user_buffer_offset;
		ret = vm_insert_page(vma, user_page_addr, page[0]);
		if (ret) {
			pr_err("%d: binder_alloc_buf failed to map page at %lx in userspace\n",
			       proc->pid, user_page_addr);
			goto err_vm_insert_page_failed;
		}
		/* vm_insert_page does not seem to increment the refcount */
	}
	if (mm) {
		up_write(&mm->mmap_sem);
		mmput(mm);
	}
	return 0;
free_range:
	for (page_addr = end - PAGE_SIZE; page_addr >= start;
	     page_addr -= PAGE_SIZE) {
		page = &proc->pages[(page_addr - proc->buffer) / PAGE_SIZE];
		if (vma)
			zap_page_range(vma, (uintptr_t)page_addr +
				proc->user_buffer_offset, PAGE_SIZE, NULL);
err_vm_insert_page_failed:
		unmap_kernel_range((unsigned long)page_addr, PAGE_SIZE);
err_map_kernel_failed:
		__free_page(*page);
		*page = NULL;
err_alloc_page_failed:
		;
	}
err_no_vma:
	if (mm) {
		up_write(&mm->mmap_sem);
		mmput(mm);
	}
	return -ENOMEM;
}

**分析一波**
此方法是内存映射的直接执行者，分别将申请的物理page内存映射到进程的用户虚拟地址（ vm_insert_page(vma, user_page_addr, page[0]);）和内核虚拟地址（map_kernel_range_noflush((unsigned long)page_addr,PAGE_SIZE, PAGE_KERNEL, page);）。
